{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from transformers import BertTokenizer\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import pandas as pd\r\n",
    "import torch.nn as nn\r\n",
    "from transformers import BertModel\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "import sys\r\n",
    "sys.path.append(\"../Public/\")\r\n",
    "from models import SentimentClassifier, BertDataset\r\n",
    "from utils import get_accuracy,full_permutation\r\n",
    "import requests\r\n",
    "import time\r\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immutable parameters\r\n",
    "device = \"cuda\"\r\n",
    "criterion = nn.BCEWithLogitsLoss()\r\n",
    "dataName = \"all\"\r\n",
    "model_dict_name = f\"{dataName}-best.pt\"\r\n",
    "apiKey = \"E8qq5xWZ95iyQZB6hSezKV\"\r\n",
    "title =f\"{dataName} notebook completed!\"\r\n",
    "NUM_WORKERS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 导入模型\r\n",
    "model = SentimentClassifier()\r\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = OrderedDict({\n",
    "'BATCH_SIZES':[128],\n",
    "'MAX_LENS':[64,128],\n",
    "'LEARNING_RATES':[0.005,0.01,0.5],\n",
    "\"EPOCHSS\":[100]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = full_permutation(hyperParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 2 combs:\n",
      "Comb1 starting: BATCH_SIZE: 128 MAX_LEN: 32 LEARNING_RATE: 0.005 EPOCHS: 1\n",
      "C1/C2: train 1/1 epochs Loss: 0.581288, Acc: 0.669714, times: 13.42s\n",
      "C1/C2: val   1/1 epochs Loss: 0.530961, Acc: 0.703742, Best_Acc: 0.703742, times: 7.77s\n",
      "C1/C2: Best_Acc: 0.703742, combs_Best_Acc: 0.703742, times: 34.39s\n",
      "Comb2 starting: BATCH_SIZE: 128 MAX_LEN: 32 LEARNING_RATE: 0.005 EPOCHS: 2\n",
      "C2/C2: train 1/2 epochs Loss: 0.539081, Acc: 0.715536, times: 13.35s\n",
      "C2/C2: val   1/2 epochs Loss: 0.542381, Acc: 0.692843, Best_Acc: 0.692843, times: 8.33s\n",
      "C2/C2: train 2/2 epochs Loss: 0.534240, Acc: 0.718362, times: 13.36s\n",
      "C2/C2: val   2/2 epochs Loss: 0.519155, Acc: 0.735349, Best_Acc: 0.735349, times: 7.79s\n",
      "C2/C2: Best_Acc: 0.735349, combs_Best_Acc: 0.735349, times: 55.82s\n",
      "2 Combs Best accuracy: 0.735349, All times: 90.21s\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "start_time = time.time()\r\n",
    "combs_best_acc = 0.0\r\n",
    "n_combs = len(table)\r\n",
    "\r\n",
    "\r\n",
    "print(f\"all {n_combs} combs:\")\r\n",
    "\r\n",
    "for comb in range(n_combs):\r\n",
    "\r\n",
    "    BATCH_SIZE = int(table.loc[comb,'BATCH_SIZE'])\r\n",
    "    MAX_LEN =  int(table.loc[comb,'MAX_LEN'])\r\n",
    "    LEARNING_RATE = table.loc[comb,'LEARNING_RATE']\r\n",
    "    EPOCHS =  int(table.loc[comb,'EPOCHS'])\r\n",
    "\r\n",
    "    print(f\"Comb{comb+1} starting: BATCH_SIZE: {BATCH_SIZE} MAX_LEN: {MAX_LEN} LEARNING_RATE: {LEARNING_RATE} EPOCHS: {EPOCHS}\")\r\n",
    "    combs_start_time = time.time()\r\n",
    "\r\n",
    "    # 加载数据\r\n",
    "    train_set = BertDataset(f'../Data/{dataName}/train.csv', maxlen = MAX_LEN )\r\n",
    "    val_set = BertDataset(f'../Data/{dataName}/val.csv', maxlen = MAX_LEN )\r\n",
    "    train_loader = DataLoader(train_set, batch_size = BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\r\n",
    "    val_loader = DataLoader(val_set, batch_size = BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\r\n",
    "    # 优化器\r\n",
    "    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\r\n",
    "    best_val_acc = 0\r\n",
    "\r\n",
    "    for epoch in range(EPOCHS):\r\n",
    "\r\n",
    "        train_loss = 0.0\r\n",
    "        train_acc=0.0\r\n",
    "        val_loss=0\r\n",
    "        val_acc=0.0\r\n",
    "        epoch_start_time = time.time()\r\n",
    "\r\n",
    "        model.train()\r\n",
    "        for i,data in enumerate(train_loader):\r\n",
    "            input_ids,attention_mask,labels=[elem.to(device) for elem in data]\r\n",
    "            #优化器置零\r\n",
    "            optimizer.zero_grad()\r\n",
    "            #得到模型的结果\r\n",
    "            out=model(input_ids,attention_mask)\r\n",
    "            #计算误差\r\n",
    "            loss=criterion(out.squeeze(-1),labels.float())\r\n",
    "            train_loss += loss.item()\r\n",
    "            #误差反向传播\r\n",
    "            loss.backward()\r\n",
    "            #更新模型参数\r\n",
    "            optimizer.step()\r\n",
    "            #计算acc \r\n",
    "            out=out.detach().cpu().numpy()\r\n",
    "            labels=labels.detach().cpu().numpy()\r\n",
    "            train_acc+=get_accuracy(out,labels)\r\n",
    "        \r\n",
    "        train_acc /= len(train_loader)\r\n",
    "        train_loss /= len(train_loader)\r\n",
    "        print(f\"C{comb+1}/C{n_combs}: train {epoch+1}/{EPOCHS} epochs Loss: {train_loss:3.6f}, Acc: {train_acc:3.6f}, times: {(time.time()-epoch_start_time):.2f}s\")\r\n",
    "\r\n",
    "\r\n",
    "        epoch_start_time = time.time()\r\n",
    "        model.eval()\r\n",
    "        with torch.no_grad():\r\n",
    "            for j,batch in enumerate(val_loader):\r\n",
    "                val_input_ids,val_attention_mask,val_labels=[elem.to(device) for elem in batch]\r\n",
    "                pred=model(val_input_ids,val_attention_mask)\r\n",
    "                loss=criterion(pred.squeeze(-1),val_labels.float())\r\n",
    "                pred=pred.detach().cpu().numpy()\r\n",
    "                val_labels=val_labels.detach().cpu().numpy()\r\n",
    "                val_acc += get_accuracy(pred,val_labels)\r\n",
    "                val_loss += loss.item()\r\n",
    "\r\n",
    "        val_acc /= len(val_loader)\r\n",
    "        val_loss /= len(val_loader)\r\n",
    "       \r\n",
    "        if val_acc > best_val_acc:\r\n",
    "            best_val_acc = val_acc\r\n",
    "            best_state_dict = model.state_dict()\r\n",
    "            best_val_loss = val_loss\r\n",
    "\r\n",
    "        print(f\"C{comb+1}/C{n_combs}: val   {epoch+1}/{EPOCHS} epochs Loss: {val_loss:3.6f}, Acc: {val_acc:3.6f}, Best_Acc: {best_val_acc:3.6f}, times: {(time.time()-epoch_start_time):.2f}s\")\r\n",
    "\r\n",
    "    \r\n",
    "    table.loc[comb,'LOSS'] = round(best_val_loss,6)\r\n",
    "    table.loc[comb,'BEST_ACC'] = round(best_val_acc,6)\r\n",
    "\r\n",
    "    if best_val_acc > combs_best_acc:\r\n",
    "        combs_best_acc = best_val_acc \r\n",
    "        combs_best_sd = best_state_dict\r\n",
    "        combs_best_loss = best_val_loss\r\n",
    "\r\n",
    "    print(f\"C{comb+1}/C{n_combs}: Best_Acc: {best_val_acc:3.6f}, combs_Best_Acc: {combs_best_acc:3.6f}, times: {(time.time()-combs_start_time):.2f}s\")\r\n",
    "\r\n",
    "print(f\"{n_combs} Combs Best accuracy: {combs_best_acc:3.6f}, All times: {(time.time()-start_time):.2f}s\")\r\n",
    "\r\n",
    "torch.save(combs_best_sd, model_dict_name)\r\n",
    "table.to_csv(f\"{dataName}-combs.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\r\n",
    "del train_loader\r\n",
    "del val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = table['BEST_ACC'].argmax()\r\n",
    "MAX_LEN = int(table.iloc[index]['MAX_LEN'])\r\n",
    "BATCH_SIZE = int(table.iloc[index]['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = ['Yelp','sst2','imdb']\r\n",
    "result_df = pd.DataFrame(columns=['Loss','Acc'],index=names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_data_name):\r\n",
    "    test_set = BertDataset(f'../Data/{test_data_name}/test.csv', maxlen = MAX_LEN )\r\n",
    "    test_loader = DataLoader(test_set, batch_size = BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\r\n",
    "\r\n",
    "    model = SentimentClassifier()\r\n",
    "    model.load_state_dict(torch.load(model_dict_name))\r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    test_acc = 0\r\n",
    "    test_loss = 0\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for j,batch in enumerate(test_loader):\r\n",
    "            test_input_ids,test_attention_mask,test_labels=[elem.to(device) for elem in batch]\r\n",
    "            pred=model(test_input_ids,test_attention_mask)\r\n",
    "            loss=criterion(pred.squeeze(-1),test_labels.float())\r\n",
    "            pred=pred.detach().cpu().numpy()\r\n",
    "            test_labels=test_labels.detach().cpu().numpy()\r\n",
    "            test_acc += get_accuracy(pred,test_labels)\r\n",
    "            test_loss += loss.item()\r\n",
    "\r\n",
    "    test_acc /= len(test_loader)\r\n",
    "    test_loss /= len(test_loader)\r\n",
    "\r\n",
    "    print(f'on {test_data_name} test set: Loss: {test_acc:3.6f} , Acc: {test_acc:3.6f}')  \r\n",
    "\r\n",
    "    return [round(test_loss,6),round(test_acc,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on Yelp test set: Loss: 0.760352 , Acc: 0.760352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on sst2 test set: Loss: 0.843930 , Acc: 0.843930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on imdb test set: Loss: 0.686719 , Acc: 0.686719\n"
     ]
    }
   ],
   "source": [
    "for name in names_list:\r\n",
    "    result_df.loc[name] = test_model(name)\r\n",
    "    \r\n",
    "result_df.to_csv(f\"{dataName}-result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParamslist = ['BATCH_SIZE','MAX_LEN','LEARNING_RATE',\"EPOCHS\"]\n",
    "all_table = pd.read_csv(f\"{dataName}-combsAll.csv\")\n",
    "all_table = all_table.append(table)\n",
    "all_table = all_table.groupby(hyperParamslist).mean()\n",
    "all_table = all_table.reset_index(hyperParamslist)\n",
    "all_table.sort_values(hyperParamslist,inplace=True)\n",
    "all_table.to_csv(f\"{dataName}-combsAll.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebookName = f\"Bert-{dataName}\"\r\n",
    "context = f\"notebook {notebookName} :  is completed\"\r\n",
    "requests.get(f\"https://api.day.app/{apiKey}/{title}/{context}?sound=anticipate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4876ff34b70794a54711585a56035755ce9dd6f9a98e80662ac35fb37c287d01"
  },
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
