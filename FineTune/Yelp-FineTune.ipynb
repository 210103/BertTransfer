{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from transformers import BertTokenizer\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import pandas as pd\r\n",
    "import torch.nn as nn\r\n",
    "from transformers import BertModel\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "import sys\r\n",
    "sys.path.append(\"../Public/\")\r\n",
    "from models import SentimentClassifier, BertDataset\r\n",
    "from utils import get_accuracy\r\n",
    "import requests\r\n",
    "import time\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immutable parameters\n",
    "device = \"cuda\"\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "dataName = \"Yelp\"\n",
    "fileName = f\"{dataName}-FineTune\"\n",
    "targetData = \"imdb\"\n",
    "model_dict_name = f\"../{dataName}/{dataName}-best.pt\"\n",
    "save_dict_name = f\"{fileName}-best.pt\"\n",
    "apiKey = \"E8qq5xWZ95iyQZB6hSezKV\"\n",
    "title =f\"{fileName} notebook completed!\"\n",
    "NUM_WORKERS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模型\r\n",
    "model = SentimentClassifier()\r\n",
    "model = model.to(device)\r\n",
    "model.load_state_dict(torch.load(model_dict_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAX_LEN = 256\n",
    "LEARNING_RATE = 0.002\n",
    "EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据集和优化器\r\n",
    "train_set = BertDataset(f'../Data/{targetData}/train.csv', maxlen = MAX_LEN )\r\n",
    "val_set = BertDataset(f'../Data/{targetData}/val.csv', maxlen = MAX_LEN )\r\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\r\n",
    "val_loader = DataLoader(val_set, batch_size = BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\r\n",
    "test_set = BertDataset(f'../Data/{targetData}/test.csv', maxlen = MAX_LEN )\r\n",
    "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\r\n",
    "# 优化器\r\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = [fileName]\r\n",
    "result_df = pd.DataFrame(columns=['V_Loss','V_Acc','T_Loss','T_Acc'],index=names_list)\r\n",
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1/4 epochs Loss: 0.451176, Acc: 0.779757, times: 144.23s\n",
      "val   1/4 epochs Loss: 0.397007, Acc: 0.820312, times: 38.28s\n",
      "1/4 epochs: Best_Acc: 0.820312\n",
      "train 2/4 epochs Loss: 0.408582, Acc: 0.804439, times: 144.18s\n",
      "val   2/4 epochs Loss: 0.396509, Acc: 0.818945, times: 36.92s\n",
      "2/4 epochs: Best_Acc: 0.820312\n",
      "train 3/4 epochs Loss: 0.406386, Acc: 0.806777, times: 144.54s\n",
      "val   3/4 epochs Loss: 0.377993, Acc: 0.819922, times: 37.29s\n",
      "3/4 epochs: Best_Acc: 0.820312\n",
      "train 4/4 epochs Loss: 0.400311, Acc: 0.808569, times: 145.17s\n",
      "val   4/4 epochs Loss: 0.382539, Acc: 0.840430, times: 36.45s\n",
      "4/4 epochs: Best_Acc: 0.840430\n",
      "Yelp-FineTune on imdb 4: Best_Acc: 0.840430, times: 735.44s\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\r\n",
    "start_time = time.time()\r\n",
    "\r\n",
    "for epoch in range(EPOCHS):\r\n",
    "\r\n",
    "    train_loss = 0.0\r\n",
    "    train_acc=0.0\r\n",
    "    val_loss=0\r\n",
    "    val_acc=0.0\r\n",
    "    epoch_start_time = time.time()\r\n",
    "\r\n",
    "    model.train()\r\n",
    "    for i,data in enumerate(train_loader):\r\n",
    "        input_ids,attention_mask,labels=[elem.to(device) for elem in data]\r\n",
    "        #优化器置零\r\n",
    "        optimizer.zero_grad()\r\n",
    "        #得到模型的结果\r\n",
    "        out=model(input_ids,attention_mask)\r\n",
    "        #计算误差\r\n",
    "        loss=criterion(out.squeeze(-1),labels.float())\r\n",
    "        train_loss += loss.item()\r\n",
    "        #误差反向传播\r\n",
    "        loss.backward()\r\n",
    "        #更新模型参数\r\n",
    "        optimizer.step()\r\n",
    "        #计算acc \r\n",
    "        out=out.detach().cpu().numpy()\r\n",
    "        labels=labels.detach().cpu().numpy()\r\n",
    "        train_acc+=get_accuracy(out,labels)\r\n",
    "    \r\n",
    "\r\n",
    "    train_acc /= len(train_loader)\r\n",
    "    train_loss /= len(train_loader)\r\n",
    "    print(f\"train {epoch+1}/{EPOCHS} epochs Loss: {train_loss:3.6f}, Acc: {train_acc:3.6f}, times: {(time.time()-epoch_start_time):.2f}s\")\r\n",
    "\r\n",
    "\r\n",
    "    epoch_start_time = time.time()\r\n",
    "    model.eval()\r\n",
    "    with torch.no_grad():\r\n",
    "        for j,batch in enumerate(val_loader):\r\n",
    "            val_input_ids,val_attention_mask,val_labels=[elem.to(device) for elem in batch]\r\n",
    "            pred=model(val_input_ids,val_attention_mask)\r\n",
    "            loss=criterion(pred.squeeze(-1),val_labels.float())\r\n",
    "            pred=pred.detach().cpu().numpy()\r\n",
    "            val_labels=val_labels.detach().cpu().numpy()\r\n",
    "            val_acc += get_accuracy(pred,val_labels)\r\n",
    "            val_loss += loss.item()\r\n",
    "\r\n",
    "    val_acc /= len(val_loader)\r\n",
    "    val_loss /= len(val_loader)\r\n",
    "    \r\n",
    "    if val_acc > best_val_acc:\r\n",
    "        best_val_acc = val_acc\r\n",
    "        best_val_loss = val_loss\r\n",
    "        best_state_dict = model.state_dict()\r\n",
    "\r\n",
    "    print(f\"val   {epoch+1}/{EPOCHS} epochs Loss: {val_loss:3.6f}, Acc: {val_acc:3.6f}, times: {(time.time()-epoch_start_time):.2f}s\")\r\n",
    "    print(f\"{epoch+1}/{EPOCHS} epochs: Best_Acc: {best_val_acc:3.6f}\")\r\n",
    "    \r\n",
    "\r\n",
    "torch.save(best_state_dict,save_dict_name)\r\n",
    "print(f\"{fileName} on {targetData} {EPOCHS}: Best_Acc: {best_val_acc:3.6f}, times: {(time.time()-start_time):.2f}s\")\r\n",
    "\r\n",
    "\r\n",
    "result_df.loc[fileName,'V_Loss']=best_val_loss\r\n",
    "result_df.loc[fileName,'V_Acc']=best_val_acc\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on imdb test set: Loss: 0.856445 , Acc: 0.856445\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier()\r\n",
    "model.load_state_dict(torch.load(save_dict_name))\r\n",
    "model.to(device)\r\n",
    "model.eval()\r\n",
    "test_acc = 0\r\n",
    "test_loss = 0\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    for j,batch in enumerate(test_loader):\r\n",
    "        test_input_ids,test_attention_mask,test_labels=[elem.to(device) for elem in batch]\r\n",
    "        pred=model(test_input_ids,test_attention_mask)\r\n",
    "        loss=criterion(pred.squeeze(-1),test_labels.float())\r\n",
    "        pred=pred.detach().cpu().numpy()\r\n",
    "        test_labels=test_labels.detach().cpu().numpy()\r\n",
    "        test_acc += get_accuracy(pred,test_labels)\r\n",
    "        test_loss += loss.item()\r\n",
    "\r\n",
    "test_acc /= len(test_loader)\r\n",
    "test_loss /= len(test_loader)\r\n",
    "\r\n",
    "print(f'on {targetData} test set: Loss: {test_acc:3.6f} , Acc: {test_acc:3.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[fileName,'T_Loss']=test_loss\r\n",
    "result_df.loc[fileName,'T_Acc']=test_acc\r\n",
    "result_df.to_csv(f\"{fileName}-result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebookName = f\"Bert {fileName}\"\r\n",
    "context = f\"notebook {notebookName} :  is completed\"\r\n",
    "requests.get(f\"https://api.day.app/{apiKey}/{title}/{context}?sound=anticipate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4876ff34b70794a54711585a56035755ce9dd6f9a98e80662ac35fb37c287d01"
  },
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
