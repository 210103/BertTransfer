{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from transformers import BertTokenizer\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import pandas as pd\r\n",
    "import torch.nn as nn\r\n",
    "from transformers import BertModel\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "import sys\r\n",
    "sys.path.append(\"../Public/\")\r\n",
    "from models import SentimentClassifier, BertDataset\r\n",
    "from utils import get_accuracy,full_permutation\r\n",
    "import requests\r\n",
    "import time\r\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\r\n",
    "criterion = nn.BCEWithLogitsLoss()\r\n",
    "dataName = \"Yelp\"\r\n",
    "model_dict_name = f\"{dataName}-best.pt\"\r\n",
    "apiKey = \"E8qq5xWZ95iyQZB6hSezKV\"\r\n",
    "title =f\"{dataName} notebook completed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 导入模型\r\n",
    "model = SentimentClassifier()\r\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParams = OrderedDict({\n",
    "'BATCH_SIZES':[64],\n",
    "'MAX_LENS':[128],\n",
    "'LEARNING_RATES':[1e-5,1e-4,1e-3],\n",
    "\"EPOCHSS\":[100]\n",
    "})\n",
    "table = full_permutation(hyperParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "combs_best_acc = 0.0\n",
    "n_combs = len(table)\n",
    "\n",
    "\n",
    "print(f\"all {n_combs} combs:\")\n",
    "\n",
    "for comb in range(n_combs):\n",
    "\n",
    "    BATCH_SIZE = int(table.loc[comb,'BATCH_SIZE'])\n",
    "    MAX_LEN =  int(table.loc[comb,'MAX_LEN'])\n",
    "    LEARNING_RATE = table.loc[comb,'LEARNING_RATE']\n",
    "    EPOCHS =  int(table.loc[comb,'EPOCHS'])\n",
    "\n",
    "    print(f\"Comb{comb+1} starting: BATCH_SIZE: {BATCH_SIZE} MAX_LEN: {MAX_LEN} LEARNING_RATE: {LEARNING_RATE} EPOCHS: {EPOCHS}\")\n",
    "    combs_start_time = time.time()\n",
    "\n",
    "    # 加载数据\n",
    "    train_set = BertDataset(f'../Data/{dataName}/train.csv', maxlen = MAX_LEN )\n",
    "    val_set = BertDataset(f'../Data/{dataName}/val.csv', maxlen = MAX_LEN )\n",
    "    train_loader = DataLoader(train_set, batch_size = BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_set, batch_size = BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\n",
    "    # 优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc=0.0\n",
    "        val_loss=0\n",
    "        val_acc=0.0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        for i,data in enumerate(train_loader):\n",
    "            input_ids,attention_mask,labels=[elem.to(device) for elem in data]\n",
    "            #优化器置零\n",
    "            optimizer.zero_grad()\n",
    "            #得到模型的结果\n",
    "            out=model(input_ids,attention_mask)\n",
    "            #计算误差\n",
    "            loss=criterion(out.squeeze(-1),labels.float())\n",
    "            train_loss += loss.item()\n",
    "            #误差反向传播\n",
    "            loss.backward()\n",
    "            #更新模型参数\n",
    "            optimizer.step()\n",
    "            #计算acc \n",
    "            out=out.detach().cpu().numpy()\n",
    "            labels=labels.detach().cpu().numpy()\n",
    "            train_acc+=get_accuracy(out,labels)\n",
    "        \n",
    "        train_acc /= len(train_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"C{comb+1}/C{n_combs}: train {epoch+1}/{EPOCHS} epochs Loss: {train_loss:3.6f}, Acc: {train_acc:3.6f}, times: {(time.time()-epoch_start_time):.2f}s\")\n",
    "\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for j,batch in enumerate(val_loader):\n",
    "                val_input_ids,val_attention_mask,val_labels=[elem.to(device) for elem in batch]\n",
    "                pred=model(val_input_ids,val_attention_mask)\n",
    "                loss=criterion(pred.squeeze(-1),val_labels.float())\n",
    "                pred=pred.detach().cpu().numpy()\n",
    "                val_labels=val_labels.detach().cpu().numpy()\n",
    "                val_acc += get_accuracy(pred,val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_acc /= len(val_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "       \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state_dict = model.state_dict()\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        print(f\"C{comb+1}/C{n_combs}: val   {epoch+1}/{EPOCHS} epochs Loss: {val_loss:3.6f}, Acc: {val_acc:3.6f}, Best_Acc: {best_val_acc:3.6f}, times: {(time.time()-epoch_start_time):.2f}s\")\n",
    "\n",
    "    \n",
    "    table.loc[comb,'LOSS'] = round(best_val_loss,6)\n",
    "    table.loc[comb,'BEST_ACC'] = round(best_val_acc,6)\n",
    "\n",
    "    if best_val_acc > combs_best_acc:\n",
    "        combs_best_acc = best_val_acc \n",
    "        combs_best_sd = best_state_dict\n",
    "        combs_best_loss = best_val_loss\n",
    "\n",
    "    print(f\"C{comb+1}/C{n_combs}: Best_Acc: {best_val_acc:3.6f}, combs_Best_Acc: {combs_best_acc:3.6f}, times: {(time.time()-combs_start_time):.2f}s\")\n",
    "\n",
    "print(f\"{n_combs} Combs Best accuracy: {combs_best_acc:3.6f}, All times: {(time.time()-start_time):.2f}s\")\n",
    "\n",
    "torch.save(combs_best_sd, model_dict_name)\n",
    "table.to_csv(f\"{dataName}-combs.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\r\n",
    "del train_loader\r\n",
    "del val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = table['BEST_ACC'].argmax()\r\n",
    "MAX_LEN = int(table.loc[index,'MAX_LEN'])\r\n",
    "BATCH_SIZE = int(table.loc[index,'BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = [dataName]\r\n",
    "result_df = pd.DataFrame(columns=['Loss','Acc'],index=names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = BertDataset(f'../Data/{dataName}/test.csv', maxlen = MAX_LEN )\r\n",
    "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifier()\n",
    "model.load_state_dict(torch.load(model_dict_name))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_acc = 0\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j,batch in enumerate(test_loader):\n",
    "        test_input_ids,test_attention_mask,test_labels=[elem.to(device) for elem in batch]\n",
    "        pred=model(test_input_ids,test_attention_mask)\n",
    "        loss=criterion(pred.squeeze(-1),test_labels.float())\n",
    "        pred=pred.detach().cpu().numpy()\n",
    "        test_labels=test_labels.detach().cpu().numpy()\n",
    "        test_acc += get_accuracy(pred,test_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_acc /= len(test_loader)\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f'on {dataName} test set: Loss: {test_acc:3.6f} , Acc: {test_acc:3.6f}')\n",
    "\n",
    "result_df.loc[dataName] = [round(test_loss,6),round(test_acc,6)]\n",
    "result_df.to_csv(f\"{dataName}-result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParamslist = ['BATCH_SIZE','MAX_LEN','LEARNING_RATE',\"EPOCHS\"]\n",
    "all_table = pd.read_csv(f\"{dataName}-combsAll.csv\",index_col=0)\n",
    "all_table = all_table.append(table)\n",
    "all_table = all_table.groupby(hyperParamslist).mean()\n",
    "all_table = all_table.reset_index(hyperParamslist)\n",
    "all_table.sort_values(hyperParamslist,inplace=True)\n",
    "all_table.to_csv(f\"{dataName}-combsAll.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookName = f\"Bert-{dataName}\"\r\n",
    "context = f\"notebook {notebookName} :  is completed\"\r\n",
    "requests.get(f\"https://api.day.app/{apiKey}/{title}/{context}?sound=anticipate\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4876ff34b70794a54711585a56035755ce9dd6f9a98e80662ac35fb37c287d01"
  },
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
